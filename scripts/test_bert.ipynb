{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e86b4f4b287972b2c5783c099d33ee10e775822b850f5f360c7edf12403ae40c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load the Pretrained BERT model...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/yuahe/projects/OntoAlign-py\")\n",
    "from ontoalign.embeds import PretrainedBert\n",
    "from ontoalign.onto import Ontology\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "bert = PretrainedBert(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "source": [
    "Check if the bert batch embeddings is equivalent to sentence by sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sents = [\"donald trump\", \"donald j trump\", \"us president\", \"apple tree\", \"pineapple fruits\", \"he is a man\", \"he is a male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeds, _ = bert.batch_word_embeds(batch_sents, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-4.5709e-06) 1.0\n",
      "tensor(9.2884e-07) 1.0\n",
      "tensor(6.7205e-06) 0.99999994\n",
      "tensor(-1.0286e-05) 1.0\n",
      "tensor(-1.9894e-05) 0.9999998\n",
      "tensor(5.5869e-06) 0.99999976\n",
      "tensor(2.9111e-05) 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch_sents)):\n",
    "    sent = [batch_sents[i]]\n",
    "    word_embeds = bert.batch_word_embeds(sent, -2)[0].squeeze(0)\n",
    "    sent_len = word_embeds.shape[0]\n",
    "    # display(word_embeds.shape, batch_embeds[i].shape)\n",
    "    print(torch.sum(batch_embeds[i][:sent_len] - word_embeds), cosine_similarity(batch_embeds[i][:sent_len], word_embeds).diagonal().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sent_embeds = bert.batch_sent_embeds_last_2_mean(batch_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-1.1625e-06, dtype=torch.float64) 0.9999999999999678\n",
      "tensor(-6.7429e-07, dtype=torch.float64) 0.99999999999996\n",
      "tensor(2.2473e-06, dtype=torch.float64) 0.9999999999999638\n",
      "tensor(-3.0763e-06, dtype=torch.float64) 0.9999999999999628\n",
      "tensor(-4.1638e-06, dtype=torch.float64) 0.9999999999999739\n",
      "tensor(1.6505e-06, dtype=torch.float64) 0.9999999999999691\n",
      "tensor(3.6689e-06, dtype=torch.float64) 0.9999999999999681\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch_sents)):\n",
    "    sent = [batch_sents[i]]\n",
    "    word_embeds = bert.batch_word_embeds(sent, -2)[0].squeeze(0)\n",
    "    sent_embed = torch.mean(word_embeds, dim=0)\n",
    "    sent_len = word_embeds.shape[0]\n",
    "    print(torch.sum(batch_sent_embeds[i] - sent_embed), cosine_similarity(batch_sent_embeds[i].unsqueeze(0), sent_embed.unsqueeze(0)).diagonal().mean())"
   ]
  },
  {
   "source": [
    "Up to now, we see that the batch algorithm brings certain computational error but it's very small (< $10^{-4}$) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_lexicon_sizes = [3, 1, 1, 2]\n",
    "batch_lexicon_sents = [\"donald trump\", \"donald j trump\", \"us president\", \"apple tree\", \"pineapple fruits\", \"he is a man\", \"he is a male\"]\n",
    "all_lexicon_sents_embeds = bert.batch_sent_embeds_last_2_mean(batch_lexicon_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.stack([torch.mean(all_lexicon_sents_embeds[:3], dim=0),\n",
    "                torch.mean(all_lexicon_sents_embeds[3:4], dim=0),\n",
    "                torch.mean(all_lexicon_sents_embeds[4:5], dim=0),\n",
    "                torch.mean(all_lexicon_sents_embeds[5:7], dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = bert.entity_embeds_from_batch_lexicon(\"batch_sent_embeds_last_2_mean\", batch_lexicon_sizes, batch_lexicon_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = torch.mean(bert.batch_sent_embeds_last_2_mean([\"donald trump\", \"donald j trump\", \"us president\"]), dim=0)\n",
    "g2 = torch.mean(bert.batch_sent_embeds_last_2_mean([\"apple tree\"]), dim=0)\n",
    "g3 = torch.mean(bert.batch_sent_embeds_last_2_mean([\"pineapple fruits\"]), dim=0)\n",
    "g4 = torch.mean(bert.batch_sent_embeds_last_2_mean([\"he is a man\", \"he is a male\"]), dim=0)\n",
    "x3 = torch.stack([g1, g2, g3, g4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.92867219, 0.9334201 , 0.87184914],\n",
       "       [0.92867219, 1.        , 0.93917666, 0.85035664],\n",
       "       [0.9334201 , 0.93917666, 1.        , 0.83408974],\n",
       "       [0.87184914, 0.85035664, 0.83408974, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cosine_similarity(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.92867218, 0.93342009, 0.87184915],\n",
       "       [0.92867218, 1.        , 0.93917666, 0.85035664],\n",
       "       [0.93342009, 0.93917665, 1.        , 0.83408974],\n",
       "       [0.87184914, 0.85035662, 0.83408973, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "cosine_similarity(x1, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.92867218, 0.93342009, 0.87184915],\n",
       "       [0.92867218, 1.        , 0.93917666, 0.85035664],\n",
       "       [0.93342009, 0.93917665, 1.        , 0.83408974],\n",
       "       [0.87184914, 0.85035662, 0.83408973, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "cosine_similarity(x2, x3)"
   ]
  },
  {
   "source": [
    "Up to this point, the batch lexicon embeddings is settled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}